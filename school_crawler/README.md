# School Crawler

Crawls K12 school sites. To get the crawler running, follow the instructions below:

## Get the initial input data from NCES

To get started, first download the Common Core of Data from NCES:

https://nces.ed.gov/ccd/files.asp#Fiscal:2,LevelId:7,SchoolYearId:37,Page:1

For example, at the time of writing, the latest data is 2022-2023. Click on the "Flat and SAS Files (12.0 MB)" link under the "Directory" section, and you'll download this zip file:

https://nces.ed.gov/ccd/data/zip/ccd_sch_029_2223_w_1a_083023.zip

Extract all files, but we only need the CSV file. We have provided with one such sample file in the `input_data` directory. For the purpose of the crawler, we only care about the following columns in the CSV file:

    * `SCH_NAME`: School name
    * `WEBSITE`: School website

## Prepare the crawl queue

The websites from CSV data above may be outdated or may redirect to the a different site. In the case of outdated sites, we'll just exclude the URL from the crawl queue. We care about URL redirection because we need to figure out what is a third party or first party. For example, the CSV data may show a school having a website `http://www.schoolname.org`, but the site may redirect to `https://www.schoolname.statename.us`. In this case, `schoolname.statename.us` will be the hostname of the school (or in some cases the district). If a URL seen linked from the site does not contain the hostname above, then it is not the first party.

To prepare the crawl queue, run the following command from the root of the project:

```python

python -m venv env
source env/bin/activate
pip install -r requirements.txt
cd school_cralwer
python prepare_crawl_queue.py input_data/ccd_sch_029_2223_w_1a_083023.csv 2024-02-28.sqlite
```

This will create a SQLite database called `2024-02-28.sqlite` in the `school_crawler` directory. This database will contain the URLs to be crawled, as well as the contents of the URLs. We recommend naming the SQLite database with the date of the first crawl. This is one way to keep track of the crawl versions.


## Run the crawler

To run the crawler, run the following command from the root of the project:

```python
python -m venv env
source env/bin/activate
cd school_cralwer
python crawl.py
```

Feel free to stop or resume the crawler at any time. The crawler will pick up where it left off.

## Analyzing third parties

This Python script, titled `analyze_third_parties.py`, is designed for analyzing third-party content based on data from a database generated by a web crawling process, with the analysis results being stored in a new database. Below is a revised overview of the script's functionality, emphasizing the refined definitions of `school_name`, `base_hostname`, and `service_hostname` within the output database schema.

### Input Parameters:
- **`<crawl_db_path>`**: The path to the input SQLite database generated from a web crawl, containing tables and records related to crawled webpages, including URLs and their redirection targets.
- **`<analysis_output_db_path>`**: The path for the output SQLite database where the analysis results, including the refined hostname data and DNS information, are stored.

### Output Database Schema:
The script establishes or updates two principal tables in the output database:

1. **`hostname` Table**: Captures distinct hostnames identified from URLs within the crawl database, organized as follows:
   - `school_name`: Refers to the name of the school or school district associated with the URL, reflecting the educational context of the crawl.
   - `base_hostname`: Indicates the hostname of the main website for the school or school district. This hostname excludes any subdomains or third-party domains, focusing strictly on the primary domain.
   - `service_hostname`: Represents a URL linked from the main website. This URL might include a hostname that is a first-party with a different subdomain or a third-party subdomain, extending the analysis to various web services connected to the main site.

2. **`hostname_dns_info` Table**: Stores DNS information for each unique `service_hostname`, detailed as follows:
   - `hostname`: The hostname for which DNS information is retrieved.
   - `dns_info`: The DNS record associated with the hostname, which can include IP addresses (A records), canonical names (CNAMEs), or reverse DNS lookup results (PTR records).
   - `dns_info_type`: Specifies the type of DNS record obtained (`A`, `CNAME`, `PTR`).

### Key Relationships:
- Both tables (`hostname` and `hostname_dns_info`) are interconnected through a foreign key relationship between `service_hostname` in the `hostname` table and `hostname` in the `hostname_dns_info` table. This allows for a comprehensive joint analysis by correlating service hostnames with their respective DNS information.

### Analysis Process Overview:
1. **Initialization**: Ensures input paths are valid, initializes database connections, and sets up the database schema in the output database.
2. **Data Extraction**: Processes URLs from the crawl database to populate the `hostname` table with `school_name`, `base_hostname`, and `service_hostname`.
3. **DNS Information Gathering**: Performs DNS lookups for each unique `service_hostname` to gather `A`, `CNAME`, and `PTR` records, storing this information in the `hostname_dns_info` table.

### Data Produced:
- **Hostnames Data**: Provides a detailed listing of hostnames, categorizing them into main websites for schools or school districts (`base_hostname`) and URLs linked from these main sites (`service_hostname`). This data is crucial for identifying first-party and third-party web services.
- **DNS Records**: Delivers exhaustive DNS records for each `service_hostname`, including direct IP addresses, canonical names, and reverse DNS lookups. This DNS data is essential for understanding the infrastructure and network relationships of the analyzed hostnames.

### Usage Instructions:
To execute the script, ensure that Python and the necessary libraries are installed. Run the script via the command line with the specified input and output database paths:

```
python analyze_third_parties.py <crawl_db_path> <analysis_output_db_path>
```

This documentation provides a clear guide on how the script processes and analyzes web crawling data to reveal insightful relationships between educational institutions' main websites and their linked web services, through detailed hostname and DNS information analysis.





## Crawling webpages

The `crawl.py` script is part of a web crawling framework designed to systematically explore and record data from K-12 school websites. It leverages asynchronous programming to efficiently crawl websites listed in a given SQLite database, storing the results for further analysis. Below is a documentation update that includes details about the integration with the `cached_http_client.py` script and the structure of the data generated by the crawler.

### Input Parameter:
- **`<db_path>`**: Specifies the path to the SQLite database that will be used to store crawl data. This database must already exist or be capable of being created at the provided path.

### Database Schema:
Upon execution, `crawl.py` utilizes and contributes to a database schema that includes, but is not limited to, the following tables:
- **`crawl_queue` Table**: Contains the queue of URLs to be visited, with details about each URLâ€™s origin and crawl depth.
  - `queue_id`: A unique identifier for each queue entry.
  - `school_name`: The name of the school or district associated with the URL.
  - `base_hostname`: The hostname of the main website for the school or district, excluding subdomains and third-party domains.
  - `depth`: The crawl depth from the initial URL.
  - `url_to_visit`: The specific URL to be crawled.
  - `result_webpage_id`: A foreign key linking to the `webpage_id` column in the `webpage` table, indicating the result of visiting the URL.
  - `referral_queue_id`: The `queue_id` of the URL from which this URL was discovered.

### Integration with `cached_http_client.py`:
The `crawl.py` script interfaces with `cached_http_client.py` to manage the process of visiting URLs and capturing web content. The `cached_http_client.py` script provides functionalities to request URLs, cache responses, and parse content, with significant aspects including:

- **Webpage Caching and Parsing**:
  - `visit_url` method: Accepts a URL and returns a tuple containing the `webpage_id`, `redirected_url`, and a list of `hrefs` found on the page. This method handles redirections, checks for binary content to avoid downloading non-HTML resources, and extracts links for further crawling.
  - `webpage` Table: Stores detailed information about each visited webpage, including the original and final URLs, timestamps, HTTP status codes, raw HTML content, and extracted metadata. This table is directly utilized by the `crawl.py` script to reference crawl results.

- **Data Stored in the `webpage` Table**:
  - `webpage_id`: A unique identifier for the webpage, serving as a primary key.
  - `url_to_visit`: The URL that was initially requested.
  - `redirected_url`: The final URL after following any redirects.
  - `visit_ts`: The timestamp of the visit.
  - `status`: The HTTP status code returned by the server.
  - `raw_html`: The raw HTML content of the page.
  - `title`, `text`: The title and extracted text of the webpage.
  - `href_list_json`: A JSON-encoded list of URLs found on the webpage.
  - `metadata_dict_json`: A JSON-encoded dictionary containing extracted metadata from the webpage.

### Operational Flow:
1. **Initialization**: Sets up database connections, initializes the `crawl_queue` and related structures, and prepares the HTTP client.
2. **Crawl Execution**: Iteratively visits URLs from the `crawl_queue`, records results in the `webpage` table, and enqueues new URLs found during the crawl. This process respects the specified concurrency limits and crawl depth to manage load and scope.

### Usage:
To run the script, execute the following command from the terminal, ensuring that Python 3.7+ and required dependencies are installed:

```
python crawl.py <db_path>
```

This documentation encapsulates the operation of `crawl.py` in conjunction with `cached_http_client.py`, detailing how the system manages web crawling tasks, processes web content, and structures the resulting data for subsequent analysis or review.

## Sample SQL queries for analysis

```sql
select
  school_name,
  base_hostname,
  dns_info,
  dns_info_type
from
  hostname a
  join hostname_dns_info b on a.service_hostname = b.hostname
where
  service_hostname = hostname and
  base_hostname = service_hostname and
  dns_info_type = 'CNAME'
order by
  random()
limit
  100;
```

This SQLite query is designed to select and display data from two tables, `hostname` and `hostname_dns_info`, which are part of the analysis database produced by the `analyze_third_parties.py` script. Hereâ€™s a breakdown of what each part of the query does:

1. **Selection of Columns**: The query selects four pieces of information to display:
   - `school_name` from the `hostname` table (`a`).
   - `base_hostname` from the `hostname` table (`a`).
   - `dns_info` from the `hostname_dns_info` table (`b`).
   - `dns_info_type` from the `hostname_dns_info` table (`b`).

2. **FROM Clause with JOIN**:
   - The query operates on two tables: `hostname` (aliased as `a`) and `hostname_dns_info` (aliased as `b`).
   - These tables are joined using an inner join on their common attribute where `a.service_hostname` matches `b.hostname`. This means for each row in the `hostname` table, it will find rows in the `hostname_dns_info` table where the `service_hostname` corresponds to a `hostname`.

3. **WHERE Clause**:
   - The query filters the results to rows where several conditions are met:
     - `service_hostname = hostname`: This filters for rows where the `service_hostname` (from the `hostname` table) matches the `hostname` (from the `hostname_dns_info` table), essentially reinforcing the join condition but seems redundant in this context unless it aims to double-check for exact matches only.
     - `base_hostname = service_hostname`: This limits the results to records where the `base_hostname` is the same as `service_hostname`. This condition suggests looking for entries where the base domain of the school or school district's main website is the same as the service's hostname, implying a focus on internal or closely related services rather than external third-party services.
     - `dns_info_type = 'CNAME'`: Filters for rows where the DNS record type is a CNAME (Canonical Name). CNAME records are used to alias one domain name to another. This condition focuses the analysis on these specific DNS records.

4. **ORDER BY Clause**:
   - `random()`: The results are ordered randomly. This is likely used to get a diverse set of results from the entire dataset each time the query is executed, especially useful for sampling or when the dataset is too large to analyze in full.

5. **LIMIT Clause**:
   - The query limits the results to 100 rows. This constraint is typically used to prevent too much data from being loaded at once, making the analysis more manageable or ensuring the query runs efficiently for a quick look at a subset of data.

In summary, this query aims to randomly select 100 instances where the base hostname (indicating the main domain of the school or district) is internally redirected (via CNAME records) within the same domain (service hostname equals base hostname). It specifically focuses on situations where a canonical name DNS record exists, which could reflect internal subdomain mappings or specific configurations within the school or districtâ€™s domain architecture.



```sql
select
  queue_id,
  school_name,
  base_hostname,
  depth,
  url_to_visit,
  result_webpage_id,
  referral_queue_id
from
  crawl_queue
where
  depth > 0
  and instr(url_to_visit, base_hostname) = 0
order by
  random()
limit
  10;
```
This SQLite query is designed to select a random sample of 10 URLs from the `crawl_queue` table, specifically targeting URLs that do not belong to the school or district's main site, indicating these are potentially third-party URLs or internal pages that use a different hostname. The query's breakdown is as follows:

- **Columns Selected**: It selects several pieces of information for each entry, including:
  - `queue_id`: A unique identifier for each entry in the crawl queue.
  - `school_name`: The name of the school or district associated with the entry.
  - `base_hostname`: The hostname of the school or district's main website.
  - `depth`: Indicates how many links away from the original URL the current URL was found. A depth greater than 0 implies that the URL is not the starting URL but was discovered by following links from the initial page.
  - `url_to_visit`: The actual URL to be visited or that has been visited.
  - `result_webpage_id`: A foreign key linking to a `webpage_id` in the `webpage` table, indicating the result of visiting this URL.
  - `referral_queue_id`: The `queue_id` of the URL that led to this URL being added to the queue, allowing tracking of how this URL was discovered.

- **Conditions**:
  - `depth > 0`: This condition filters for URLs that were found by following links from the initial page(s), excluding those URLs that are the starting points of the crawl.
  - `instr(url_to_visit, base_hostname) = 0`: This function checks if the `base_hostname` (the main site's hostname) is a substring of the `url_to_visit`. The condition being equal to 0 means the `base_hostname` does not occur within the `url_to_visit`, indicating the URL is from a different hostname, which could be a third-party site or a different subdomain not matching the main site's base hostname.

- **Order and Limit**:
  - `order by random()`: This orders the results in a random manner each time the query is executed. It ensures that every execution potentially returns a different set of URLs.
  - `limit 10`: Limits the results to 10 rows. This constraint is useful for sampling or when the UI can only accommodate a limited number of entries.

In summary, this query is particularly useful for auditing or analyzing external links or third-party content linked from a school's main website. By selecting URLs that are not directly part of the main site (as indicated by the hostname difference), analysts or web administrators can evaluate the types of external content or services being linked to, which can be crucial for security assessments, content auditing, and understanding the web ecosystem surrounding educational institutions.



```sql
select
  queue_id,
  school_name,
  base_hostname,
  depth,
  url_to_visit,
  result_webpage_id,
  referral_queue_id
from
  crawl_queue
where
  depth > 0
  and instr(url_to_visit, base_hostname) = 0
  and base_hostname = "www.newton.k12.ma.us"
  and url_to_visit like '%newton.k12.ma.us%'
order by
  random()
limit
  100;
```

This SQLite query is designed to retrieve information from the `crawl_queue` table, specifically focusing on URLs that are part of the Newton Public Schools' domain but are not the main site (indicated by `base_hostname`). The query is tailored to identify subdomains and other pages that are technically part of the Newton Public Schools' web infrastructure but are hosted on different subdomains than the main website. Here's a breakdown of what the query does:

1. **Selection of Columns**: The query selects several pieces of information for each row, including the `queue_id`, `school_name`, `base_hostname`, `depth`, `url_to_visit`, `result_webpage_id`, and `referral_queue_id`. These fields provide a comprehensive view of each URL's context within the crawl process, including its depth in the crawl tree and its source (referral).

2. **Conditions for Selection**:
   - `depth > 0`: This condition filters out the main school website (usually at depth 0) and focuses on pages discovered through links from the main site or beyond.
   - `instr(url_to_visit, base_hostname) = 0`: This function checks if the `base_hostname` is part of the `url_to_visit`. A result of 0 means the `base_hostname` is not found within the `url_to_visit`, indicating that the URL is likely a subdomain or a different domain. This is crucial for finding URLs outside of the main school website's domain.
   - `base_hostname = "www.newton.k12.ma.us"`: This specifically filters for entries related to Newton Public Schools by matching entries with the base hostname of the district's main website.
   - `url_to_visit like '%newton.k12.ma.us%'`: Ensures that the URLs in question still belong to the broader domain of Newton Public Schools, even if they are not directly part of the main site. This captures subdomains and deeper links that are still officially part of the school's digital presence.

3. **Random Ordering and Limit**:
   - `order by random()`: The results are ordered randomly each time the query runs. This is useful for sampling different subdomains or pages within the Newton Public Schools' domain without bias.
   - `limit 100`: Limits the results to 100 entries, which is a practical way to manage output size and focus on a subset of the available data.

**Summary**: The query is structured to discover and list up to 100 random URLs that are part of the Newton Public Schools' web infrastructure but are not hosted on the main website's domain. It filters for depth to exclude the main page, uses string matching to exclude URLs that do not include the base hostname directly (indicating subdomains or external sites still under the school's domain), and ensures all results pertain specifically to Newton Public Schools. This provides a useful tool for analyzing the extent and diversity of the school district's online presence across different subdomains.



```
select
  count(school_name) as school_count,
  case
    when lower(dns_info) like '%sharpschool%' then 'SharpSchool'
    when lower(dns_info) like '%edlio%' then 'Edlio'
    when lower(dns_info) like '%schoolblocks%' then 'SchoolBlocks'
    when lower(dns_info) like '%apptegy%' then 'Apptegy'
    when lower(dns_info) like '%finalsite%' then 'FinalSite'
    when lower(dns_info) like '%schoolwires%' then 'SchoolWires'
    else 'Others'
  end as service_provider
from
  (
    select
      school_name,
      base_hostname,
      dns_info,
      dns_info_type
    from
      hostname a
      join hostname_dns_info b on a.service_hostname = b.hostname
    where
      service_hostname = hostname
      and base_hostname = service_hostname
      and dns_info_type = 'CNAME'
  )
group by
  service_provider
order by
  school_count desc;
```

This SQLite query is designed to analyze DNS information, specifically CNAME records, to categorize schools by their service providers based on the DNS info associated with their hostnames. The query operates in several steps:

1. **Subquery Selection**: It begins with a subquery that selects rows from a join between two tables: `hostname` (aliased as `a`) and `hostname_dns_info` (aliased as `b`). The join condition is that `a.service_hostname` matches `b.hostname`, ensuring that the analysis is focused on hostnames that have corresponding DNS info records.

2. **Filtering**: Within this joined data, it filters for rows where `service_hostname` equals `hostname`, `base_hostname` equals `service_hostname`, and the `dns_info_type` is `'CNAME'`. This effectively narrows the data down to CNAME records that match certain criteria, likely aiming to focus on direct service hostnames without considering subdomains or third-party services.

3. **Case-insensitive Search with `LOWER`**: The query then applies a `CASE` statement to categorize each school based on its `dns_info`. By converting `dns_info` to lowercase using `LOWER(dns_info)`, it performs a case-insensitive search for various keywords associated with known service providers (`'sharpschool'`, `'edlio'`, `'schoolblocks'`, `'apptegy'`, `'finalsite'`, `'schoolwires'`). Each keyword match leads to a corresponding categorical label (e.g., `'SharpSchool'`, `'Edlio'`).

4. **Aggregation and Grouping**: The outer query counts the number of schools (`count(school_name)`) for each service provider category determined by the `CASE` statement. This count is performed within groups determined by the `service_provider` alias, effectively aggregating schools by their categorized service provider.

5. **Sorting and Limiting**: Finally, the results are sorted in descending order by `school_count`, showcasing the most common service providers at the top of the list. This order allows for a quick understanding of which service providers are most prevalent within the dataset.

6. **Output**: The output of the query consists of two columns: `school_count`, representing the number of schools using each service provider, and `service_provider`, the name of the service provider or 'Others' for any DNS info that does not match the specified keywords.

This query provides insights into the distribution of web hosting and related services among schools, identifying which service providers are most commonly used based on the analysis of CNAME DNS records.